---
title: "tiers"
author: "Huey Kwik"
date: "May 30, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)
library(dplyr)
library(ggplot2)
library(mclust)
library(gsubfn)
library(XML)

source("util.R")

# Initialize constants
pos.list = c('qb','rb','wr','te','k','dst')

kYear = 2015
kCutoffs = c(QB = 24, RB = 40, WR = 60, TE = 24, K = 24, DST = 24)
kTiers = c(QB = 8, RB = 9, WR = 12, TE = 8, K = 5, DST = 6)

# Plotting
font = 3.5
barsize = 1.5  
dotsize = 2  
```

# Introduction

A couple years ago, I encountered a [New York Times article](http://nyti.ms/1t6Bgc3) 
by Boris Chen that explained a way to use machine learning to improve your starting lineup 
in fantasy football. 

Every week, legions of fantasy football experts come up with their lists of player
rankings. [FantasyPros](https://www.fantasypros.com/) aggregates these and comes up with an Expert Conensus Ranking. 
Chen applies a [Gaussian mixture model](http://bit.ly/1VKWt6S) to the rankings, 
which produces tiers of similarly ranked players. 

The tiers lend themselves to easier decision-making. For instance, one should draft
players in higher tiers versus other popular tactics, e.g. filling a specific position
early.

In this project, I show that we can improve upon these tiers by clustering based
on projected points instead of ECR. When run against the 2015 season data, my 
technique seems to perform better.

## Grabbing Data

In order to do this project, I needed the following data for the 2015 season:

* Projected and actual scores from [Fantasy Football Analytics](apps.fantasyfootballanalytics.net/projections)
* Expert Consensus Rankings from FantasyPros.

The projected scores for each week are the average of the scores from CBS, ESPN,
FantasyFootball Nerd, FantasySharks, FFToday, NFL NumberFire, and YahooSports. 
I used Yahoo! standard scoring.

I wrote a modified version of [Chen's code](https://github.com/borisachen/fftiers) 
to download the ECR data:

```{r download ECR}
download.data = function(week, pos.list) {
  for (mp in pos.list) {
 	 	# Remove old data files.
    rmold1 = paste('rm data/2015/week-', week, '-',mp,'-raw.xls', sep='')
  	system(rmold1)
  	
  	urlStr = fn$identity("http://www.fantasypros.com/nfl/rankings/`mp`.php?week=`week`&export=xls")
  	dlPath = fn$identity("data/2015/week-`week`-`mp`-raw.xls")
  	
	  rmold2 = paste('rm ~ data/2015/week_', week, '_', mp, '.tsv', sep='')
  	system(rmold2)
    download.file(urlStr, destfile=dlPath)  
    sedstr = paste("sed '1,4d' data/2015/week-", week, '-',mp,'-raw.xls', 
  			  ' > data/2015/week_', week, '_', mp, '.tsv',sep="")
    system(sedstr)
  }	  
}

# TODO(huey): Error when trying to download using knitr, but works otherwise.
# sapply(1:17, function(week) download.data(week))
```

## Evaluating Accuracy

[Tier accuracy](http://www.borischen.co/2013/11/week-10-retrospective.html) 
is defined as the percentage of time a higher tier results in a higher median score 
than a lower tier.

```{r}
computeAccuracy = function(scores, tiers) {
  n_tiers = length(unique(tiers))
  n_correct = 0
  n_comparisons = 0
  
  medians = c()
  for (i in seq(n_tiers)) {
    median = median(scores[tiers == i])
    medians = c(medians, median)
  }
  
  for (i in seq(n_tiers-1)) {
    curr_median = medians[i]
    below_medians = medians[(i+1):n_tiers]
  
    comparisons = sapply(below_medians, function(x) curr_median > x)
  
    n_correct = n_correct + sum(comparisons)
    n_comparisons = n_comparisons + length(comparisons)
  }
  
  return(n_correct / n_comparisons)
}
```

# Computing Tiers
Here I compare clusters generated from ECR to those generated from projections.

Caveats

* TierECR doesn't match Chen's completely. 
  * This is because his site only the latest
set of tiers, i.e. Week 17. 
  * He also uses visual inspection to determine the number of clusters and will
  change them each week (he publishes a new set of tiers each week)

```{r integrate_data, warning = FALSE}
tier_ecr_accuracy = data.frame()
tier_proj_accuracy = data.frame()

read_score_data = function(path) {
  sdf = read_csv(path)
  sdf$Week = week
  sdf$actualPoints[sdf$actualPoints == "null"] = 0
  sdf$actualPoints = as.numeric(sdf$actualPoints)
  return(sdf)
}
# 
# read_ecr_data = function(path) {
#   
# }

# returns a factor
computeTierProjections = function(points, k) {
  clusters = Mclust(points, G = k)
  n_clusters = length(unique(clusters$classification))
  
  # Sometimes the model isn't able to fit K clusters
  while (n_clusters == 0) {
    k = k - 1
    clusters = Mclust(points, G = k)
    n_clusters = length(unique(clusters$classification))
  }
  
  # Reversing so that highest projected points gets Tier 1 
  tiers = factor(clusters$classification)
  levels(tiers) = rev(levels(tiers))
  levels(tiers) = n_clusters:1
  
  return(tiers)
}

computeTierECRs = function(ranks, k) {
  fit = Mclust(ranks, G = k)
  tiers = factor(fit$classification)
  
  # Sometimes there is a gap between tiers, e.g. 1 1 2 2 4
  # Remove it
  levels(tiers) = 1:length(unique(fit$classification))
  
  # fit = Mclust(ecr_df$Avg.Rank, G = kTiers[pos])
  # ecr_df$TierECR = factor(fit$classification)
  #   levels(ecr_df$TierECR) = 1:length(unique(fit$classification))
  
  return(tiers)
}

plot = FALSE
for (week in seq(17)) {
  # Parser warnings are due to trailing comma at the end of each row
  sdf = read_score_data(fn$identity("data/2015/FFA-CustomRankings-Week-`week`.csv"))
  
  week_tier_ecr_accuracy = c()
  week_tier_proj_accuracy = c()
  
  for (pos in toupper(pos.list)) {
    # print(paste("Position: ", pos))
    
    ## Calculate TierProj
    pos_df = sdf %>% filter(position == pos, positionRank <= kCutoffs[pos])
    pos_df$TierProj = computeTierProjections(pos_df$points, kTiers[pos])
    
    ## Calculate TierECR
      
    # Load ECR data for the week
    ecr_df = read_tsv(fn$identity("data/2015/week_`week`_`tolower(pos)`.tsv"))
    ecr_df = ecr_df[1:kCutoffs[pos], ]  # The data is ordered from best rank to worst rank  
    names(ecr_df) = make.names(names(ecr_df))
      
    ecr_df$TierECR = computeTierECRs(ecr_df$Avg.Rank, kTiers[pos])
    
    if (plot) {
      ecr_df$nchar = nchar(as.character(ecr_df$Player.Name))  # For formatting later
      
      # Calculate position rank, negative so lowest rank will be on top in the plot
      # below
      ecr_df$position.rank = -seq(nrow(ecr_df))
      
      # We put Avg.Rank as y because geom_errorbar requires ymin/ymax. We then flip the 
      # coordinates.
      p = ggplot(ecr_df, aes(x = position.rank, y = Avg.Rank))
      p = p + geom_errorbar(aes(ymin = Avg.Rank - Std.Dev/2, ymax = Avg.Rank + Std.Dev/2, width=0.2, colour=TierECR), size=barsize*0.8, alpha=0.4)
      p = p + coord_flip()
      p = p + geom_text(aes(label=Player.Name, colour=TierECR, y = Avg.Rank - nchar/6 - Std.Dev/1.4), size=font)
      p = p + scale_x_continuous("Expert Consensus Rank")
      p = p + ylab("Average Expert Rank")
      #p
      
      # Output the plots somewhere so I can look at them
      out_dir = fn$identity("out/`kYear`/week`week`/png/")
      out_path = fn$identity("`out_dir`week-`week`-`pos`.png")
      dir.create(out_dir, recursive = TRUE)
      ggsave(file = out_path, width = 9.5, height = 8, dpi = 150)
    }
    
    if (pos == "DST") {
      ecr_scores = inner_join(ecr_df, pos_df, by = c("Team" = "team"))
    } else {
      all_pos_df = sdf %>% filter(position == pos)
      ecr_scores = inner_join(ecr_df, all_pos_df, by = c("Player.Name" = "playername"))
    }
    
    acc = computeAccuracy(pos_df$actualPoints, pos_df$TierProj)
    acc_ecr = computeAccuracy(ecr_scores$actualPoints, ecr_scores$TierECR)
    
    week_tier_proj_accuracy  = c(week_tier_proj_accuracy, acc)
    week_tier_ecr_accuracy = c(week_tier_ecr_accuracy, acc_ecr)
  }

  tier_proj_accuracy = rbind(tier_proj_accuracy, week_tier_proj_accuracy)
  tier_ecr_accuracy = rbind(tier_ecr_accuracy, week_tier_ecr_accuracy)

  names(tier_proj_accuracy) = toupper(pos.list)
  names(tier_ecr_accuracy) = toupper(pos.list)
}
```

# Results

Define: 
For consistency, "TierProjections" will refer to creating tiers based on projected
scores, and "TierECR" will refer to creating tiers based on Expert Consensus Rank. 

It appears that clustering based off of projections performs slightly better.
For instance, TierProjections performs better on QBs, RBs, WRs, TEs, and Kickers, 
whereas TierECR performs better on Defense. Note that tier accuracy for Kickers is
near 50%, which suggests streaming kickers is a good idea.

```{r}
  round(tier_proj_accuracy * 100, 1)

  round(tier_ecr_accuracy * 100, 1)

  mean_proj = sapply(tier_proj_accuracy, mean)
  mean_tier = sapply(tier_ecr_accuracy, mean)
  
  round(mean_proj * 100, 1)
  round(mean_tier * 100, 1)
```

To get a better sense of how these two algorithms compete on more data, there 
are two approaches we can take. 

The most obvious approach is to look at historical data. Unfortunately, FantasyPros
does not keep a record of ECR from past seasons. I tried using the 
[Wayback Machine](https://archive.org/web/), but there simply were not enough 
snapshots to make this worthwhile. 

Chen has published tiers for the 2013-2015 seasons on his blog, but the data isn't
always available. For instance, some of the links to the tiers in the 2014 season
are dead. However, the 2013 season is available so I might try that next.

Instead, we create bootstrap samples for the 2015 season and then see how well
each method performs.

```{r}



for score_data {
  n = 1000
  for (i in 1:n) {
    score_data = score_data[sample(nrow, nrow, replace = TRUE), ]
    compute tiers and accuracy ecr
    compute tiers and accuracy proj
    
    n_proj_wins = acc_proj > acc_ecr
    store the comparison
  }
  n_proj_wins / n 
}
```

What % of time does TierProjections perform better than TierECR?
# Random partioning into tiers

Here we explore another way to generate tiers. Instead of applying GMM on the 
list of ordered projections, we compute many different random tiers. 

Using the accuracy (as defined above) as a cost function, we pick the partition
that provides the best accuracy.

Below are the utilty functions written to compute a random partition.

```{r randomPartition utility function}

randomPartition = function(k, len) {
  indexes = sample(seq(len-1), k-1)
  return(sort(indexes))
}

expand = function(tiers, indexes, len) {
# expand(letters[1:4], c(1,2,3), 5)  # abcdd
# expand(letters[1:4], c(1,3,4), 5)  # abbcd
# expand(letters[1:4], c(1,3,4), 5)  # abbcdd
  vec = vector("list", len)
  k = length(indexes)
  start = 1
  for (i in seq(k)) {
    idx = indexes[i]
    vec[start:idx] = tiers[i]
    start = idx + 1 
  }
  vec[start:len] = tiers[i+1]
  return(unlist(vec))
}

bestPartition = function(k, act_points, order_data) {
  # Usage example: (also suggest setting seed for reproducibility)
  # test_df = ecr_scores %>% select(playername, points, actualPoints) %>% arrange(desc(points))
  # bestPartition(7, test_df$actualPoints, test_df$points)

  max_acc = -1
  best_indexes = c()
  len = length(act_points)
  for (i in seq(kTries)) {
    indexes = randomPartition(k, len)
    tier = expand(1:k, indexes, len)
    acc = computeAccuracy(act_points, tier)
    if (acc > max_acc) {
      max_acc = acc
      best_indexes = indexes
    }
  }
  return (best_indexes)
}
```

Here we compute the random partitions on the 2015 data and then save them
to a data frame.

```{r random partition tiers, warning = FALSE}
set.seed(1)
kTries = 2000  # Make smaller for debugging

tier_df = data.frame()
for (week in seq(17)) {
  # Parser warnings are due to trailing comma at the end of each row
  sdf = read_score_data(fn$identity("data/2015/FFA-CustomRankings-Week-`week`.csv"))
  for (pos in toupper(pos.list)) {
    pos_data = sdf %>% filter(position == pos, positionRank <= kCutoffs[pos]) %>% arrange(desc(points))
    best_indexes = bestPartition(kTiers[pos], pos_data$actualPoints, pos_data$points)
    tiers = expand(1:kTiers[pos], best_indexes, nrow(pos_data))
    tier_counts = sapply(1:kTiers[pos], function(tier) { c(week, pos, tier, length(tiers[tiers == tier])) })
    tier_df = rbind(tier_df, t(tier_counts))
  }
}
names(tier_df) = c("Week", "Pos", "Tier", "TierCount")
tier_df$TierCount = as.numeric(tier_df$TierCount)
```

Looking at small multiples of the tiers helps us see the distribution of tiers 
per position each week. 

Interestingly, the algorithm suggests that Tier 1 for all non Kicker/Defense 
positions is size 1.

```{r faceting by tier}
for (pos in toupper(pos.list)) {
  pos_tier = tier_df %>% filter(Pos == pos)
  print(ggplot(pos_tier, aes(x=Week, y=TierCount, group = 1)) + 
    geom_point() + facet_wrap(~Tier) + ggtitle(pos))
}
```

Now what's left is to use this data and apply it again to the 2015 season.

The idea I'll try below is to take the mode for each tier's count and then use 
that to partition the data. If there are multiple modes, I pick the higher count.

```{r mode}
qb = tier_df %>% filter(Pos == "QB")
tier = qb %>% filter(Tier == 1)
mode(tier$TierCount)

tiers_list = list()
for (pos in toupper(pos.list)) {
  pdf = tier_df %>% filter(Pos == pos)
  pos_tiers = sapply(1:kTiers[pos], function(t) {
    tdf = pdf %>% filter(Tier == t)
    modes = mode(tdf$TierCount)
    if (length(mode) > 0) {
      modes = modes[length(mode)]
    }
    return(modes)
  })
  tiers_list[pos] = list(pos_tiers)
}
```

Next, we'll apply these tiers to the entire dataset and see what our accuracy score is.

If there are still some players left over after applying these tiers, then 
we'll just create an additional tier to put them in.

```{r apply tiers from random partition, warning = FALSE}
expand_by_counts = function(tiers, counts) {
  unlist(sapply(1:length(tiers), function(i) { rep(tiers[i], counts[i]) }))
}
expand_by_counts(letters[1:4], c(1,2,1,4))  # abbcdddd

tier_part_accuracy = data.frame()
for (week in seq(17)) {
  # Parser warnings are due to trailing comma at the end of each row
  sdf = read_score_data(fn$identity("data/2015/FFA-CustomRankings-Week-`week`.csv"))
  
  week_tier_part_accuracy = c()
  
  for (pos in toupper(pos.list)) {
     pos_df = sdf %>% filter(position == pos, positionRank <= kCutoffs[pos])
     
     # apply tiers
     tiers = expand_by_counts(1:kTiers[pos], tiers_list[[pos]])
  
     # if length < kCutoffs[pos], add in an additional tier
     if (length(tiers) < nrow(pos_df)) {
       n_left = nrow(pos_df) - length(tiers)
       k = kTiers[pos] + 1
       tiers = c(tiers, rep(k, n_left))
     }
     pos_df$TierPart = tiers

     acc = computeAccuracy(pos_df$actualPoints, pos_df$TierPart)
     week_tier_part_accuracy  = c(week_tier_part_accuracy, acc)
  }

  tier_part_accuracy = rbind(tier_part_accuracy, week_tier_part_accuracy)
}
names(tier_part_accuracy) = toupper(pos.list)
mean_part = sapply(tier_part_accuracy, mean)
```

Using this random partitioning technique appears to perform worse than using
GMM on projections or ECR.

```{r}
mean_part
```

