---
title: "tiers"
author: "Huey Kwik"
date: "May 30, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)
library(dplyr)
library(ggplot2)
library(mclust)
library(gsubfn)
library(XML)
library(tictoc)

source("util.R")

# Initialize constants
pos.list = c('qb','rb','wr','te','k','dst')

kYear = 2015
kCutoffs = c(QB = 24, RB = 40, WR = 60, TE = 24, K = 24, DST = 24)
kTiers = c(QB = 8, RB = 9, WR = 12, TE = 8, K = 5, DST = 6)

# Plotting
font = 3.5
barsize = 1.5  
dotsize = 2  
```

# Introduction

A couple years ago, I encountered a [New York Times article](http://nyti.ms/1t6Bgc3) 
by Boris Chen that explained a way to use machine learning to improve your starting lineup 
in fantasy football. 

Every week, legions of fantasy football experts come up with their lists of player
rankings. [FantasyPros](https://www.fantasypros.com/) aggregates these and comes up with an Expert Conensus Ranking. 
Chen applies a [Gaussian mixture model](http://bit.ly/1VKWt6S) to the rankings, 
which produces tiers of similarly ranked players. 

The tiers lend themselves to easier decision-making. For instance, one should draft
players in higher tiers versus other popular tactics, e.g. filling a specific position
early.

In this project, I show that we can improve upon these tiers by clustering based
on projected points instead of ECR. When run against the 2015 season data, my 
technique seems to perform better.

## Grabbing Data

In order to do this project, I needed the following data for the 2015 season:

* Projected and actual scores from [Fantasy Football Analytics](apps.fantasyfootballanalytics.net/projections)
* Expert Consensus Rankings from FantasyPros.

The projected scores for each week are the average of the scores from CBS, ESPN,
FantasyFootball Nerd, FantasySharks, FFToday, NFL NumberFire, and YahooSports. 
I used Yahoo! standard scoring.

I wrote a modified version of [Chen's code](https://github.com/borisachen/fftiers) 
to download the ECR data:

```{r download ECR}
download.data = function(week, pos.list) {
  for (mp in pos.list) {
 	 	# Remove old data files.
    rmold1 = paste('rm data/2015/week-', week, '-',mp,'-raw.xls', sep='')
  	system(rmold1)
  	
  	urlStr = fn$identity("http://www.fantasypros.com/nfl/rankings/`mp`.php?week=`week`&export=xls")
  	dlPath = fn$identity("data/2015/week-`week`-`mp`-raw.xls")
  	
	  rmold2 = paste('rm ~ data/2015/week_', week, '_', mp, '.tsv', sep='')
  	system(rmold2)
    download.file(urlStr, destfile=dlPath)  
    sedstr = paste("sed '1,4d' data/2015/week-", week, '-',mp,'-raw.xls', 
  			  ' > data/2015/week_', week, '_', mp, '.tsv',sep="")
    system(sedstr)
  }	  
}

# TODO(huey): Error when trying to download using knitr, but works otherwise.
# sapply(1:17, function(week) download.data(week))
```

## Evaluating Accuracy

[Tier accuracy](http://www.borischen.co/2013/11/week-10-retrospective.html) 
is defined as the percentage of time a higher tier results in a higher median score 
than a lower tier.

```{r}
computeAccuracy = function(scores, tiers) {
  n_tiers = length(unique(tiers))
  
  # It's possible the clustering algorith does not find tiers, especially
  # on bootstrapped data.
  if (n_tiers == 1) {
    return(0)
  }
  n_correct = 0
  n_comparisons = 0
  
  medians = c()
  for (i in seq(n_tiers)) {
    median = median(scores[tiers == i])
    medians = c(medians, median)
  }
  
  for (i in seq(n_tiers-1)) {
    curr_median = medians[i]
    below_medians = medians[(i+1):n_tiers]
  
    comparisons = sapply(below_medians, function(x) curr_median > x)
  
    n_correct = n_correct + sum(comparisons)
    n_comparisons = n_comparisons + length(comparisons)
  }
  
  return(n_correct / n_comparisons)
}
```

# Computing Tiers
Here I compare clusters generated from ECR to those generated from projections.

Caveats

* TierECR doesn't match Chen's completely. 
  * This is because his site only the latest
set of tiers, i.e. Week 17. 
  * He also uses visual inspection to determine the number of clusters and will
  change them each week (he publishes a new set of tiers each week)

```{r 2015, warning = FALSE}

randomPartition = function(k, len) {
  indexes = sample(seq(len-1), k-1)
  return(sort(indexes))
}

expand = function(tiers, indexes, len) {
# expand(letters[1:4], c(1,2,3), 5)  # abcdd
# expand(letters[1:4], c(1,3,4), 5)  # abbcd
# expand(letters[1:4], c(1,3,4), 5)  # abbcdd
  vec = vector("list", len)
  k = length(indexes)
  start = 1
  for (i in seq(k)) {
    idx = indexes[i]
    vec[start:idx] = tiers[i]
    start = idx + 1 
  }
  vec[start:len] = tiers[i+1]
  return(unlist(vec))
}

bestPartition = function(k, points, tries) {
  # Usage example: (also suggest setting seed for reproducibility)
  # test_df = ecr_scores %>% select(playername, points, actualPoints) %>% arrange(desc(points))
  # bestPartition(7, test_df$actualPoints, test_df$points)

  max_acc = -1
  best_indexes = c()
  len = length(points)
  for (i in seq(tries)) {
    indexes = randomPartition(k, len)
    tier = expand(1:k, indexes, len)
    acc = computeAccuracy(points, tier)
    if (acc > max_acc) {
      max_acc = acc
      best_indexes = indexes
    }
  }
  return (best_indexes)
}

read_score_data = function(path, week) {
  sdf = read_csv(path)
  sdf$Week = week
  sdf$actualPoints[sdf$actualPoints == "null"] = 0
  sdf$actualPoints = as.numeric(sdf$actualPoints)
  return(sdf)
}

read_ecr_data = function(path) {
  ecr_df = read_tsv(path) 
  names(ecr_df) = make.names(names(ecr_df))
  return(ecr_df)
}

computeTiers = function(scores, k, reverse = FALSE) {
  # reverse: Useful for clustering on projected points. We want highest projected points to be rank 1, so 
  # we reverse the levels.
  clusters = NULL
  while (is.null(clusters)) {
    tryCatch({ 
      clusters = Mclust(scores, G = k)
      n_clusters = length(unique(clusters$classification))
    }, 
    warning = function(w) { warning(w); return(NULL) },
    error = function(e) { warning(e); return(NULL) })
    
    if (!is.null(clusters)) break 
    k = k - 1
  }
  
  n_clusters = length(unique(clusters$classification))
  tiers = factor(clusters$classification)
  if (reverse) {
    levels(tiers) = rev(levels(tiers))
    levels(tiers) = n_clusters:1
  } else {
    levels(tiers) = 1:n_clusters
  }
    
  return(tiers)
}

joinECRScores = function(ecr_df, all_pos_df, pos) {
  if (pos == "DST") {
    ecr_scores = inner_join(ecr_df, all_pos_df, by = c("Team" = "team"))
  } else {
    ecr_scores = inner_join(ecr_df, all_pos_df, by = c("Player.Name" = "playername"))
  }
  return(ecr_scores)
}

tier_ecr_accuracy = data.frame()
tier_proj_accuracy = data.frame()
tier_proj_random_accuracy = data.frame()
plot = FALSE

tic(quiet = FALSE)
set.seed(1) 
kTries = 10000  #200000
for (week in seq(17)) {
  # print(paste("Week: ", week))
  # Parser warnings are due to trailing comma at the end of each row
  sdf = read_score_data(fn$identity("data/2015/FFA-CustomRankings-Week-`week`.csv"), week)
  
  week_tier_ecr_accuracy = c()
  week_tier_proj_accuracy = c()
  week_tier_proj_random_accuracy = c()
  
  for (pos in toupper(pos.list)) {
    # print(paste("Position: ", pos))
    
    ## Calculate TierProj
    pos_df = sdf %>% filter(position == pos, positionRank <= kCutoffs[pos]) %>% arrange(desc(points))
    #pos_df = sdf %>% filter(position == pos, positionRank <= kCutoffs[pos])
    
    pos_df$TierProj = computeTiers(pos_df$points, kTiers[pos], reverse = TRUE)
    
    # debugging
    #select(pos_df, playername, points, actualPoints, positionRank, TierProj) 
    
    ## Calculate TierProjRandomPartition
    best_indexes = bestPartition(kTiers[pos], pos_df$points, kTries)
    pos_df$TierProjRandom = expand(1:kTiers[pos], best_indexes, nrow(pos_df))
    
    # Write out to files
    out_dir = fn$identity("out/`kYear`/week`week`/csv/")
    out_path = fn$identity("`out_dir`week-`week`-`pos`.csv")
    dir.create(out_dir, recursive = TRUE)
  
    write_csv(select(pos_df, playername, points, actualPoints, TierProj, TierProjRandom), out_path)
    
    ## Calculate TierECR
    ecr_df = read_ecr_data(fn$identity("data/2015/week_`week`_`tolower(pos)`.tsv"))
    ecr_df = ecr_df[1:kCutoffs[pos], ]  # The data is ordered from best rank to worst rank  
    ecr_df$TierECR = computeTiers(ecr_df$Avg.Rank, kTiers[pos])
    
    if (plot) {
      ecr_df$nchar = nchar(as.character(ecr_df$Player.Name))  # For formatting later
      
      # Calculate position rank, negative so lowest rank will be on top in the plot
      # below
      ecr_df$position.rank = -seq(nrow(ecr_df))
      
      # We put Avg.Rank as y because geom_errorbar requires ymin/ymax. We then flip the 
      # coordinates.
      p = ggplot(ecr_df, aes(x = position.rank, y = Avg.Rank))
      p = p + geom_errorbar(aes(ymin = Avg.Rank - Std.Dev/2, ymax = Avg.Rank + Std.Dev/2, width=0.2, colour=TierECR), size=barsize*0.8, alpha=0.4)
      p = p + coord_flip()
      p = p + geom_text(aes(label=Player.Name, colour=TierECR, y = Avg.Rank - nchar/6 - Std.Dev/1.4), size=font)
      p = p + scale_x_continuous("Expert Consensus Rank")
      p = p + ylab("Average Expert Rank")
      #p
      
      # Output the plots somewhere so I can look at them
      out_dir = fn$identity("out/`kYear`/week`week`/png/")
      out_path = fn$identity("`out_dir`week-`week`-`pos`.png")
      dir.create(out_dir, recursive = TRUE)
      ggsave(file = out_path, width = 9.5, height = 8, dpi = 150)
    }
    
    all_pos_df = sdf %>% filter(position == pos)
    ecr_scores = joinECRScores(ecr_df, all_pos_df, pos)
    
    acc_proj = computeAccuracy(pos_df$actualPoints, pos_df$TierProj)
    acc_proj_random = computeAccuracy(pos_df$actualPoints, pos_df$TierProjRandom)
    acc_ecr = computeAccuracy(ecr_scores$actualPoints, ecr_scores$TierECR)
    
    week_tier_proj_accuracy  = c(week_tier_proj_accuracy, acc_proj)
    week_tier_ecr_accuracy = c(week_tier_ecr_accuracy, acc_ecr)
    week_tier_proj_random_accuracy = c(week_tier_proj_random_accuracy, acc_proj_random)
  }

  tier_proj_accuracy = rbind(tier_proj_accuracy, week_tier_proj_accuracy)
  tier_ecr_accuracy = rbind(tier_ecr_accuracy, week_tier_ecr_accuracy)
  tier_proj_random_accuracy = rbind(tier_proj_random_accuracy, week_tier_proj_random_accuracy)

  names(tier_proj_accuracy) = toupper(pos.list)
  names(tier_proj_random_accuracy) = toupper(pos.list)
  names(tier_ecr_accuracy) = toupper(pos.list)
}
toc()
```

# Results

Define: 
For consistency, "TierProjections" will refer to creating tiers based on projected
scores, and "TierECR" will refer to creating tiers based on Expert Consensus Rank. 

It appears that clustering based off of projections performs slightly better for
all positions. Note that tier accuracy for Kickers is near 50%, which suggests 
that streaming kickers is a good idea.

```{r}
  round(tier_proj_accuracy * 100, 1)
  round(tier_ecr_accuracy * 100, 1)
  round(tier_proj_random_accuracy * 100, 1)

  mean_proj = sapply(tier_proj_accuracy, mean)
  mean_tier = sapply(tier_ecr_accuracy, mean)
  mean_proj_random = sapply(tier_proj_random_accuracy, mean)
  
  round(mean_proj * 100, 1)
  round(mean_tier * 100, 1)
  round(mean_proj_random * 100, 1)
  
  # # run this
  # round(tier_proj_random_accuracy * 100, 1)
  # mean_proj_random = sapply(tier_proj_random_accuracy, mean)
  # round(mean_proj_random * 100, 1)
```

To get a better sense of how these two algorithms compete on more data, there 
are two approaches we can take. 

The most obvious approach is to look at historical data. Unfortunately, FantasyPros
does not keep a record of ECR from past seasons. I tried using the 
[Wayback Machine](https://archive.org/web/), but there simply were not enough 
snapshots to make this worthwhile. 

Chen has published tiers for the 2013-2015 seasons on his blog, but the data isn't
always available. For instance, some of the links to the tiers in the 2014 season
are dead. However, the 2013 season is available so I might try that next.

Instead, we create bootstrap samples for the 2015 season and then see how well
each method performs.

```{r bootstrapping}

bootstrapCompare = function(n) {
  results_df = data.frame()
  
  for (week in seq(17)) {
    sdf = read_score_data(fn$identity("data/2015/FFA-CustomRankings-Week-`week`.csv"), week)
    row = c()
    for (pos in toupper(pos.list)) {
      n_proj_wins = 0
      pos_df = sdf %>% filter(position == pos)
      ecr_df = read_ecr_data(fn$identity("data/2015/week_`week`_`tolower(pos)`.tsv"))
      ecr_scores = joinECRScores(ecr_df, pos_df, pos) 
      for (i in seq(n)) {
        sample = ecr_scores[sample(nrow(ecr_scores), nrow(ecr_scores), replace = TRUE), ]
    
        #pos_df = sample %>% filter(positionRank <= kCutoffs[pos])
        pos_df = sample %>% arrange(desc(points))
        pos_df = pos_df[1:kCutoffs[pos], ]
        # select(pos_df, Player.Name, points, actualPoints)  # For debugging
        pos_df$TierProj = computeTiers(pos_df$points, kTiers[pos], reverse = TRUE)
        # select(pos_df, Player.Name, points, actualPoints, TierProj)
        
        ## Calculate TierECR
        ecr_df = sample %>% arrange(Rank)
        ecr_df = ecr_df[1:kCutoffs[pos], ]
        #select(ecr_df, Player.Name, points, actualPoints, Avg.Rank)  # For debugging
        ecr_df$TierECR = computeTiers(ecr_df$Avg.Rank, kTiers[pos])
    
        acc = computeAccuracy(pos_df$actualPoints, pos_df$TierProj)
        
        acc_ecr = computeAccuracy(ecr_df$actualPoints, ecr_df$TierECR)
        
        n_proj_wins = n_proj_wins + (acc > acc_ecr) 
      }
      row = c(row, n_proj_wins / n)
    }
    results_df = rbind(results_df, row)
  }
  names(results_df) = toupper(pos.list)
  return(results_df)
}
```

Interesting conclusion from bootstrapping: it seems like GMM based on projections
does worse! It does better for RBs and WRs, but worse on QBs/TEs/Ks, and substantially
worse on Defense.

```{r bootstrap results}
set.seed(1); n1 = bootstrapCompare(1)  #  w
# set.seed(1); n10 = bootstrapCompare(10)  # w
# set.seed(1); n50 = bootstrapCompare(50)  # w
set.seed(1); n100 = bootstrapCompare(100)  #
# set.seed(1); n200 = bootstrapCompare(200) # w
# set.seed(1); n500 = bootstrapCompare(500) # w
set.seed(1); n1000 = bootstrapCompare(1000)
set.seed(1); n10000 = bootstrapCompare(1000)

# sapply(n1000, mean)
#       QB        RB        WR        TE         K       DST 
#0.4953529 0.5107647 0.5514706 0.5035882 0.4959412 0.3998824 
```

What % of time does TierProjections perform better than TierECR?
# Random partioning into tiers

Here we explore another way to generate tiers. Instead of applying GMM on the 
list of ordered projections, we compute many different random tiers. 

Using the accuracy (as defined above) as a cost function, we pick the partition
that provides the best accuracy.

Below are the utilty functions written to compute a random partition.


